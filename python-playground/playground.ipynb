{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2.extensions import connection\n",
    "from mes_proto_python.proto import offers_pb2\n",
    "from google.protobuf.json_format import Parse\n",
    "from typing import List\n",
    "\n",
    "def read_proto_from_db(connection: connection) -> List[offers_pb2.Offer]:\n",
    "    result = []\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            query = \"\"\"SELECT details::TEXT FROM store.offers;\"\"\"\n",
    "            cursor.execute(query)\n",
    "            rows = cursor.fetchall()\n",
    "            for row in rows:\n",
    "                new_offer = offers_pb2.Offer()\n",
    "                Parse(row[0], new_offer)\n",
    "                result.append(new_offer)\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        connection.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "def decimal_number_to_float(price: offers_pb2.Money):\n",
    "    if price.value < 1000:\n",
    "        price.value\n",
    "    return price.value * 10 ** -price.scale\n",
    "\n",
    "\n",
    "def offer_to_flat_dict(offer: offers_pb2.Offer) -> Dict[str, Any]:\n",
    "    raw_price = decimal_number_to_float(offer.apartment.price.value)\n",
    "    raw_size = decimal_number_to_float(offer.apartment.size)\n",
    "    return {\n",
    "        'title': offer.title,\n",
    "        'description': offer.description,\n",
    "        'time_scraped': offer.time_scraped.ToDatetime(),\n",
    "        'price': raw_price if offer.apartment.price.value.value >= 1000 else raw_price / 100,\n",
    "        'size': raw_size if raw_size > 5 else raw_size * 100,\n",
    "        'address': offer.apartment.address,\n",
    "        'latitude': offer.apartment.location.latitude,\n",
    "        'longitude': offer.apartment.location.longitude,\n",
    "        'year_built': offer.apartment.year_built if offer.apartment.year_built != 0 else None,\n",
    "        'room_count': offer.apartment.room_count,\n",
    "        'floor': offer.apartment.floor,\n",
    "    }\n",
    "\n",
    "\n",
    "def proto_list_to_dict_list(list: List[offers_pb2.Offer]):\n",
    "    result = []\n",
    "    for offer in list:\n",
    "        try:\n",
    "            result.append(offer_to_flat_dict(offer))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing offer: {e}\")\n",
    "            continue  # This will move to the next item in the loop\n",
    "    return result\n",
    "\n",
    "\n",
    "def proto_list_to_dataframe(list: List[offers_pb2.Offer]): \n",
    "    return pd.DataFrame(proto_list_to_dict_list(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [price]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from mes_proto_python.proto import offers_pb2\n",
    "from google.protobuf.json_format import Parse\n",
    "\n",
    "connection = psycopg2.connect(\n",
    "    dbname='admin',\n",
    "    user='admin',\n",
    "    password='mysecretpassword',\n",
    "    host='localhost',\n",
    "    port='6020'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_proto = read_proto_from_db(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_dataframe = proto_list_to_dataframe(offers_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def clear_dataframe(offers_dataframe):\n",
    "    warsaw_only = offers_dataframe[((offers_dataframe['latitude'].between(52, 53.2)) & (offers_dataframe['longitude'].between(20, 22)))]\n",
    "    only_numeric_values = warsaw_only.drop(['time_scraped', 'title', 'description', 'address'], axis=1)\n",
    "    only_numeric_values.loc[only_numeric_values['room_count'] > 50, 'room_count'] = np.nan\n",
    "    only_numeric_values.loc[only_numeric_values['room_count'] == 0, 'room_count'] = np.nan\n",
    "    only_numeric_values.loc[only_numeric_values['year_built'] < 1800, 'year_built'] = np.nan\n",
    "    for col in only_numeric_values.columns:\n",
    "        only_numeric_values[col + '_is_nan'] = only_numeric_values[col].isna().astype(int)\n",
    "    return only_numeric_values\n",
    "\n",
    "def clear_dataframe2(offers_dataframe):\n",
    "    clear = clear_dataframe(offers_dataframe)\n",
    "    return clear[clear['price'] <= 10_000_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "clean_dataframe = clear_dataframe(offers_dataframe)\n",
    "sns.pairplot(clean_dataframe[['latitude', 'longitude', 'size', 'price', 'year_built', 'room_count', 'floor']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataframe):\n",
    "    train_dataset = dataframe.sample(frac=0.8, random_state=0)\n",
    "    test_dataset = dataframe.drop(train_dataset.index)\n",
    "\n",
    "    train_features = train_dataset.copy()\n",
    "    test_features = test_dataset.copy()\n",
    "\n",
    "    train_label = train_features.pop('price')\n",
    "    test_label = test_features.pop('price')\n",
    "\n",
    "    return train_features, train_label, train_dataset, test_features, test_label, test_dataset\n",
    "\n",
    "\n",
    "train_features, train_label, train_dataset, test_features, test_label, test_dataset = split(clean_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "metrics=[\n",
    "    'mae',\n",
    "    'mape',\n",
    "    r_squared,\n",
    "    tf.metrics.RootMeanSquaredError(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MeanImputerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, data, **kwargs):\n",
    "        super(MeanImputerLayer, self).__init__(**kwargs)\n",
    "        mean_data = data.mean().values\n",
    "        self.mean_values = tf.Variable(mean_data, trainable=False, dtype=tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.where(tf.math.is_nan(inputs), self.mean_values, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_features, train_label, optimizer, loss_function, epochs, metrics):\n",
    "    linear_model = tf.keras.Sequential([\n",
    "        MeanImputerLayer(train_features),\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    linear_model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)\n",
    "\n",
    "    linear_model_history = linear_model.fit(\n",
    "        train_features,\n",
    "        train_label,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        validation_split = 0.2)\n",
    "    \n",
    "    return linear_model, linear_model_history\n",
    "\n",
    "\n",
    "def evaluate(model, test_features, test_label):\n",
    "    loss, *metrics = model.evaluate(test_features, test_label)\n",
    "    result = {\n",
    "        'loss': loss,\n",
    "        'mae': metrics[0],\n",
    "        'mape': metrics[1],\n",
    "        'r_squared': metrics[2],\n",
    "        'root_mean_squared_error': metrics[3]\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam\n",
    "loss_functions = [\n",
    "    'mean_squared_error',\n",
    "    'mean_absolute_error'\n",
    "]\n",
    "epochs = [\n",
    "    2_000,\n",
    "    5_000,\n",
    "    10_000\n",
    "]\n",
    "learning_rates = [\n",
    "    0.05,\n",
    "    0.2,\n",
    "    0.4,\n",
    "    0.5\n",
    "]\n",
    "\n",
    "results_lr = {}\n",
    "\n",
    "for loss_function in loss_functions:\n",
    "    for epoch in epochs:\n",
    "        for learning_rate in learning_rates:\n",
    "            print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            model, _ = train_model(train_features.copy(), train_label.copy(), optimizer(learning_rate=learning_rate), loss_function, epoch, metrics)\n",
    "            print(optimizer, loss_function, epoch, learning_rate)\n",
    "            results_lr[f\"{optimizer}, {loss_function}, {epoch}, {learning_rate}\"] = evaluate(model, test_features, test_label)\n",
    "            print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_lr = sorted(results_lr, key=lambda k: results_lr[k]['mae'])\n",
    "for key in sorted_keys_lr:\n",
    "    print(f\"Model: {key}\")\n",
    "    for metric, value in results_lr[key].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_dnn(train_features, train_label, optimizer, loss_function, epochs, metrics, activation):\n",
    "    linear_model_dnn = tf.keras.Sequential([\n",
    "        MeanImputerLayer(train_features),\n",
    "        tf.keras.layers.Dense(128, activation=activation),\n",
    "        tf.keras.layers.Dense(128, activation=activation),\n",
    "        tf.keras.layers.Dense(128, activation=activation),\n",
    "        tf.keras.layers.Dense(units=1)\n",
    "    ])\n",
    "\n",
    "    linear_model_dnn.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)\n",
    "\n",
    "    linear_model_history = linear_model_dnn.fit(\n",
    "        train_features,\n",
    "        train_label,\n",
    "        epochs=epochs,\n",
    "        verbose=0,\n",
    "        validation_split = 0.2)\n",
    "    \n",
    "    return linear_model_dnn, linear_model_history\n",
    "\n",
    "\n",
    "def evaluate_dnn(model, test_features, test_label):\n",
    "    loss, *metrics = model.evaluate(test_features, test_label)\n",
    "    result = {\n",
    "        'loss': loss,\n",
    "        'mae': metrics[0],\n",
    "        'mape': metrics[1],\n",
    "        'r_squared': metrics[2],\n",
    "        'root_mean_squared_error': metrics[3]\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    # tf.keras.optimizers.legacy.SGD,\n",
    "    tf.keras.optimizers.legacy.RMSprop,\n",
    "    tf.keras.optimizers.legacy.Adam,\n",
    "    tf.keras.optimizers.legacy.Adagrad,\n",
    "    # tf.keras.optimizers.legacy.Adadelta,\n",
    "    # tf.keras.optimizers.legacy.Ftrl\n",
    "]\n",
    "loss_functions = [\n",
    "    'mean_squared_error',\n",
    "    'mean_absolute_error'\n",
    "]\n",
    "epochs = [\n",
    "    1_000,\n",
    "    2_000,\n",
    "    5_000,\n",
    "    10_000\n",
    "]\n",
    "learning_rates = [\n",
    "    0.01,\n",
    "    0.1,\n",
    "    0.3,\n",
    "    0.5\n",
    "]\n",
    "activation_functions = [\n",
    "    \"elu\",\n",
    "    \"exponential\",\n",
    "    \"relu\",\n",
    "    \"selu\",\n",
    "    \"sigmoid\",\n",
    "    \"softmax\",\n",
    "    \"softplus\",\n",
    "    \"softsign\",\n",
    "    \"tanh\",\n",
    "    \"swish\"\n",
    "]\n",
    "\n",
    "results_dnn = {}\n",
    "\n",
    "for loss_function in loss_functions:\n",
    "    for optimizer in optimizers:\n",
    "        for epoch in epochs:\n",
    "            for activation_function in activation_functions:\n",
    "                for learning_rate in learning_rates:\n",
    "                    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                    model, _ = train_model_dnn(train_features.copy(), train_label.copy(), optimizer(learning_rate=learning_rate), loss_function, epoch, metrics, activation_function)\n",
    "                    print(optimizer, loss_function, epoch, learning_rate)\n",
    "                    results_dnn[f\"{optimizer}, {loss_function}, {epoch}, {learning_rate} {activation_function}\"] = evaluate_dnn(model, test_features.copy(), test_label.copy())\n",
    "                    print('\\n\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_dnn = sorted(results_dnn, key=lambda k: results_dnn[k]['mae'])\n",
    "for key in sorted_keys_dnn:\n",
    "    print(f\"Model: {key}\")\n",
    "    for metric, value in results_dnn[key].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "def train_model_rf(train_dataset, metrics, num_trees, max_num_nodes, max_depth, split_axis, growing_strategy):\n",
    "    train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_dataset, label='price', task=tfdf.keras.Task.REGRESSION)\n",
    "\n",
    "    model_rf = tfdf.keras.RandomForestModel(\n",
    "        num_trees=num_trees,\n",
    "        max_depth=max_depth,\n",
    "        max_num_nodes=max_num_nodes,\n",
    "        split_axis=split_axis,\n",
    "        growing_strategy=growing_strategy,\n",
    "        task=tfdf.keras.Task.REGRESSION,\n",
    "    )\n",
    "\n",
    "    model_rf.compile(metrics=metrics)\n",
    "\n",
    "    linear_model_history = model_rf.fit(train_ds, verbose=0)\n",
    "    \n",
    "    return model_rf, linear_model_history\n",
    "\n",
    "\n",
    "def evaluate_rf(model, test_dataset):\n",
    "    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_dataset, label='price', task=tfdf.keras.Task.REGRESSION)\n",
    "    loss, *metrics = model.evaluate(test_ds)\n",
    "    result = {\n",
    "        'loss': loss,\n",
    "        'mae': metrics[0],\n",
    "        'mape': metrics[1],\n",
    "        'r_squared': metrics[2],\n",
    "        'root_mean_squared_error': metrics[3]\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_trees = [100, 300, 900, 1200, 2000] \n",
    "max_depths = [4, 8, 16, 32, 64, 128, 512, 1024, 256] # growing_strategy = LOCAL\n",
    "max_nums_nodes = [4, 16, 64, 128, 256, 512, 1024, 2048, 5096] # growing_strategy = BEST_FIRST_GLOBAL\n",
    "splits_axis = ['AXIS_ALIGNED', 'SPARSE_OBLIQUE'] \n",
    "\n",
    "results_rf = {}\n",
    "\n",
    "for num_trees in nums_trees:\n",
    "    for max_depth in max_depths:\n",
    "            for split_axis in splits_axis:\n",
    "                print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                model, _ = train_model_rf(train_dataset.copy(), metrics, num_trees, None, max_depth, split_axis, 'LOCAL')\n",
    "                results_rf[f\"{num_trees}, {max_depth}, {split_axis}, LOCAL\"] = evaluate_rf(model, test_dataset.copy())\n",
    "                print('\\n\\n\\n')\n",
    "            \n",
    "for num_trees in nums_trees:\n",
    "    for max_num_nodes in max_nums_nodes:\n",
    "            for split_axis in splits_axis:\n",
    "                print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                model, _ = train_model_rf(train_dataset.copy(), metrics, num_trees, max_num_nodes, None, split_axis, 'BEST_FIRST_GLOBAL')\n",
    "                results_rf[f\"{num_trees}, {max_num_nodes}, {split_axis}, BEST_FIRST_GLOBAL\"] = evaluate_rf(model, test_dataset.copy())\n",
    "                print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_rf = sorted(results_rf, key=lambda k: results_rf[k]['mae'])\n",
    "for key in sorted_keys_rf:\n",
    "    print(f\"Model: {key}\")\n",
    "    for metric, value in results_rf[key].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "def train_model_gbt(train_dataset, metrics, num_trees, max_num_nodes, max_depth, split_axis, growing_strategy):\n",
    "    train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_dataset, label='price', task=tfdf.keras.Task.REGRESSION)\n",
    "\n",
    "    model_gbt = tfdf.keras.GradientBoostedTreesModel(\n",
    "        num_trees=num_trees,\n",
    "        max_depth=max_depth,\n",
    "        max_num_nodes=max_num_nodes,\n",
    "        split_axis=split_axis,\n",
    "        growing_strategy=growing_strategy,\n",
    "        task=tfdf.keras.Task.REGRESSION,\n",
    "    )\n",
    "\n",
    "    model_gbt.compile(metrics=metrics)\n",
    "\n",
    "    linear_model_history = model_gbt.fit(train_ds, verbose=0)\n",
    "    \n",
    "    return model_gbt, linear_model_history\n",
    "\n",
    "\n",
    "def evaluate_gbt(model, test_dataset):\n",
    "    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_dataset, label='price', task=tfdf.keras.Task.REGRESSION)\n",
    "    loss, *metrics = model.evaluate(test_ds)\n",
    "    result = {\n",
    "        'loss': loss,\n",
    "        'mae': metrics[0],\n",
    "        'mape': metrics[1],\n",
    "        'r_squared': metrics[2],\n",
    "        'root_mean_squared_error': metrics[3]\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_trees = [100, 300, 900, 1200, 2000] \n",
    "max_depths = [4, 8, 16, 32, 64, 128, 512, 1024, 256] # growing_strategy = LOCAL\n",
    "max_nums_nodes = [4, 16, 64, 128, 256, 512, 1024, 2048, 5096] # growing_strategy = BEST_FIRST_GLOBAL\n",
    "splits_axis = ['AXIS_ALIGNED', 'SPARSE_OBLIQUE'] \n",
    "\n",
    "results_gbt = {}\n",
    "\n",
    "for num_trees in nums_trees:\n",
    "    for max_depth in max_depths:\n",
    "            for split_axis in splits_axis:\n",
    "                print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                model, _ = train_model_gbt(train_dataset.copy(), metrics, num_trees, None, max_depth, split_axis, 'LOCAL')\n",
    "                results_gbt[f\"{num_trees}, {max_depth}, {split_axis}, LOCAL\"] = evaluate_gbt(model, test_dataset.copy())\n",
    "                print('\\n\\n\\n')\n",
    "            \n",
    "for num_trees in nums_trees:\n",
    "    for max_num_nodes in max_nums_nodes:\n",
    "            for split_axis in splits_axis:\n",
    "                print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                model, _ = train_model_gbt(train_dataset.copy(), metrics, num_trees, max_num_nodes, None, split_axis, 'BEST_FIRST_GLOBAL')\n",
    "                results_gbt[f\"{num_trees}, {max_num_nodes}, {split_axis}, BEST_FIRST_GLOBAL\"] = evaluate_gbt(model, test_dataset.copy())\n",
    "                print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_gbd = sorted(results_gbt, key=lambda k: results_gbt[k]['r_squared'])\n",
    "for key in sorted_keys_gbd:\n",
    "    print(f\"Model: {key}\")\n",
    "    for metric, value in results_gbt[key].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K N Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "def train_model_knn(train_features, train_label, n_neighbors, weights, algorithm, metric):\n",
    "    train_features.fillna(train_features.mean(), inplace=True)\n",
    "\n",
    "    knn_regressor = KNeighborsRegressor(\n",
    "        n_neighbors=n_neighbors,\n",
    "        weights=weights,\n",
    "        metric=metric,\n",
    "        algorithm=algorithm\n",
    "    )\n",
    "    history = knn_regressor.fit(train_features, train_label)\n",
    "    \n",
    "    return knn_regressor, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_knn(model, test_features, test_label):\n",
    "    test_features.fillna(test_features.mean(), inplace=True)\n",
    "    y_true = test_label\n",
    "\n",
    "    y_pred = model.predict(test_features)\n",
    "\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    result = {\n",
    "        'mae': mae,\n",
    "        'mape': mape,\n",
    "        'r_squared': r2,\n",
    "        'root_mean_squared_error': rmse\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_knn = {}\n",
    "\n",
    "for n_neighbors in [3, 5, 10, 30, 50]:\n",
    "    for weights in ['uniform', 'distance']:\n",
    "        for metric in ['cityblock', 'cosine', 'euclidean', 'haversine', 'nan_euclidean']:\n",
    "            for algorithm in ['auto', 'ball_tree', 'kd_tree', 'brute']:\n",
    "                try:\n",
    "                    model, _ = train_model_knn(train_features.copy(), train_label.copy(), n_neighbors, weights, algorithm, metric)\n",
    "                    result_knn[f'n_neighbors: {n_neighbors}, weights: {weights}, metric: {metric}, algorithm: {algorithm}'] = \\\n",
    "                        evaluate_knn(model, test_features.copy(), test_label.copy())\n",
    "                except Exception as e:\n",
    "                    print('Outcome: error', e)\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_keys_knn = sorted(result_knn, key=lambda k: result_knn[k]['r_squared'])\n",
    "for key in sorted_keys_knn:\n",
    "    print(f\"Model: {key}\")\n",
    "    for metric, value in result_knn[key].items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"-----------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
